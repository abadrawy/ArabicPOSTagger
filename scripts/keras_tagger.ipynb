{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import feature_extract as f\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.metrics import accuracy_score\n",
    "import to_word2vec as w\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        #X_batch = X[batch_index,:]\n",
    "       \n",
    "        y_batch = y[batch_index]  #52083 9370 59700\n",
    "        counter += 1       \n",
    "        print(\"len\",X_batch.shape[0])\n",
    "        yield np.reshape(X_batch,(batch_size,1,f)), y_batch\n",
    "        #yield X_batch,y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "try random bactch gernator\n",
    "use a fixed number of steps per epoch \n",
    "\"\"\"\n",
    "def kerasMLP():\n",
    "    featuers_train,feauters_test,labels_train,labels_test,feature_names =f.split_data(False)\n",
    "    #featuers_train,feauters_test,feature_names=f.featureSelection(featuers_train,feauters_test,labels_train)\n",
    "\n",
    "    #tr=featuers_train.shape[0]\n",
    "    #te=feauters_test.shape[0]\n",
    "    #dims = len(feature_names)\n",
    "    \n",
    "    dims=2000\n",
    "    #dims=52380\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    transfoemed_labels_train = le.fit_transform(labels_train)\n",
    "    hot_labels_train = np_utils.to_categorical(transfoemed_labels_train, 8)\n",
    "    transformed_labels_test=le.fit_transform(labels_test)\n",
    "    hot_labels_test = np_utils.to_categorical(transformed_labels_test, 8)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #nerual  model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=dims, init=\"uniform\",\n",
    "    \tactivation=\"softmax\")) #input layer\n",
    "    model.add(Dense(64)) #hidden layerS\n",
    "    model.add(Dense(32)) #hidden layerS\n",
    "\n",
    "    model.add(Dense(8)) #output layer\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    \n",
    "    #ftting model\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "    \tmetrics=[\"accuracy\"])\n",
    "    \n",
    "    \"\"\"model.fit_generator(generator=batch_generator(featuers_train,hot_labels_train,10000,False),\n",
    "                    epochs=30,\n",
    "                   steps_per_epoch=tr/10000)\"\"\"\n",
    "    \n",
    "   \n",
    "    model.fit(featuers_train, hot_labels_train, nb_epoch=30, batch_size=300)\n",
    "\n",
    "\n",
    "    #testing \n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "    \n",
    "    #(loss, accuracy) =model.evaluate_generator(generator=batch_generator(feauters_test, hot_labels_test,1000,False),steps=te/1000)\n",
    "    (loss, accuracy) =  model.evaluate(feauters_test, hot_labels_test,\n",
    "    \tbatch_size=50, verbose=1)\n",
    "    print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    \t\n",
    "    accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def kerasRNN():\n",
    "  \n",
    "    featuers_train,feauters_test,labels_train,labels_test,f_names=f.split_data(False)\n",
    "    #featuers_train,feauters_test,feature_names=f.featureSelection(featuers_train,feauters_test,labels_train)\n",
    "\n",
    "    featuers_train=featuers_train[:30000]\n",
    "    feauters_test=feauters_test[:8685]\n",
    "    labels_train=labels_train[:30000]\n",
    "    labels_test=labels_test[:8685]\n",
    "\n",
    "    tr=featuers_train.shape[0]\n",
    "    te=feauters_test.shape[0]\n",
    "    #print(\"theree\",tr,te)\n",
    "    #featuers_train,feauters_test,f_names=f.victorize( featuers_train,feauters_test)\n",
    "    dims = len(f_names)    #dims=500\n",
    "    #dims=52380\n",
    "    #dims=2000\n",
    "    #global f\n",
    "\n",
    "    #f=dims\n",
    "\n",
    "    #print(\"theree\",tr,te,dims)\n",
    "\n",
    "    #featuers_train=np.reshape( featuers_train,(len(featuers_train),1,dims))\n",
    "    #feauters_test=np.reshape( feauters_test,(len(feauters_test),1,dims))\n",
    "    \n",
    "   \n",
    "    le = LabelEncoder()\n",
    "    transfoemed_labels_train = le.fit_transform(labels_train)\n",
    "    hot_labels_train = np_utils.to_categorical(transfoemed_labels_train,8)\n",
    "    transformed_labels_test=le.fit_transform(labels_test)\n",
    "    hot_labels_test = np_utils.to_categorical(transformed_labels_test, 8)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, return_sequences=True,\n",
    "               input_shape=(None,dims)))  # returns a sequence of vectors of dimension 32\n",
    "\n",
    "    model.add(Bidirectional(LSTM(64,return_sequences=True)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(32))) # return a single vector of dimension 32\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    \n",
    "    #ftting model\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    #sgd = SGD(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='RMSProp',\n",
    "    \tmetrics=[\"accuracy\"])\n",
    "\n",
    "   \n",
    "    model.fit_generator(generator=batch_generator(featuers_train,hot_labels_train,5000,False),\n",
    "                    epochs=30,\n",
    "                   steps_per_epoch=tr/5000)   \n",
    "    #model.fit(featuers_train, hot_labels_train, nb_epoch=30, batch_size=300)\n",
    "    #testing \n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "\n",
    "    (loss, accuracy) =model.evaluate_generator(generator=batch_generator(feauters_test, hot_labels_test,1737,False),steps=te/1737)\n",
    "    #(loss, accuracy) =  model.evaluate(feauters_test, hot_labels_test,\n",
    "    \t#batch_size=50, verbose=1)\n",
    "    print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    \t\n",
    "    accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor train_f ,train_l in  zip(transformed_featuers_train,labels_trainn):\\n       hot_labels_train=[]\\n       train_re= np.reshape( train_f,(1,len(train_f),dims))\\n       temp=le.fit_transform(train_l)\\n       hot_labels_train=hot_labels_train.append(np_utils.to_categorical(temp,8))\\n       #model.train_on_batch(train_re,hot_labels_train)\\n       model.fit(train_re,hot_labels_train, epochs=1, batch_size=1,shuffle=False)\\n       #model.reset_states()\\n    #test the model\\n    print(\"[INFO] evaluating on testing set...\")\\n    #(loss, accuracy) = model.evaluate(test_re, hot_labels_test,\\n    \\t#batch_size=1, verbose=1)\\n    #model.reset_states()\\n    for test_f ,test_l in  zip(transformed_featuers_test,labels_testn):\\n       hot_labels_test=[]\\n       test_re= np.reshape( test_f,(1,len(test_f),dims))\\n       temp=le.fit_transform(test_l)\\n       hot_labels_test=hot_labels_test.append(np_utils.to_categorical(temp,8))\\n       (loss, accuracy)=model.evaluate(test_re, hot_labels_test,\\n    \\tbatch_size=1, verbose=1)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kerasMLPV():\n",
    "    transformed_featuers_train,labels_trainn,transformed_featuers_test,labels_testn=w.prepare_data()\n",
    "    transformed_featuers_train=np.array(transformed_featuers_train)\n",
    "    transformed_featuers_test=np.array(transformed_featuers_test)\n",
    "    \n",
    "    #dims = len(feature_names)\n",
    "    \n",
    "    dims=300\n",
    "    le = LabelEncoder()\n",
    "    transfoemed_labels_train = le.fit_transform(labels_trainn)\n",
    "    hot_labels_train = np_utils.to_categorical(transfoemed_labels_train,8)\n",
    "    transformed_labels_test=le.fit_transform(labels_testn)\n",
    "    hot_labels_test = np_utils.to_categorical(transformed_labels_test, 8)\n",
    "    \n",
    "    #nerual  model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dims, input_dim=dims, init=\"uniform\",\n",
    "    \tactivation=\"softmax\")) #input layer\n",
    "    #model.add(Dense(128)) #hidden layerS\n",
    "    model.add(Dense(64,use_bias=True)) #hidden layerS\n",
    "    #model.add(Dense(32)) #hidden layerS\n",
    "    #model.add(Dense(16)) #hidden layerS\n",
    "    model.add(Dense(8)) #output layer\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    #ftting model\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    #sgd = SGD(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam',\n",
    "    \tmetrics=[\"accuracy\"])\n",
    "    model.fit(transformed_featuers_train, hot_labels_train, nb_epoch=30, batch_size=300,shuffle=False)\n",
    "    \n",
    "    #testing \n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "    (loss, accuracy) =  model.evaluate(transformed_featuers_test, hot_labels_test,\n",
    "    \tbatch_size=50, verbose=1)\n",
    "    print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    \t\n",
    "    accuracy * 100))\n",
    "    \n",
    "\"\"\"\n",
    "for train_f ,train_l in  zip(transformed_featuers_train,labels_trainn):\n",
    "       hot_labels_train=[]\n",
    "       train_re= np.reshape( train_f,(1,len(train_f),dims))\n",
    "       temp=le.fit_transform(train_l)\n",
    "       hot_labels_train=hot_labels_train.append(np_utils.to_categorical(temp,8))\n",
    "       #model.train_on_batch(train_re,hot_labels_train)\n",
    "       model.fit(train_re,hot_labels_train, epochs=1, batch_size=1,shuffle=False)\n",
    "       #model.reset_states()\n",
    "    #test the model\n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "    #(loss, accuracy) = model.evaluate(test_re, hot_labels_test,\n",
    "    \t#batch_size=1, verbose=1)\n",
    "    #model.reset_states()\n",
    "    for test_f ,test_l in  zip(transformed_featuers_test,labels_testn):\n",
    "       hot_labels_test=[]\n",
    "       test_re= np.reshape( test_f,(1,len(test_f),dims))\n",
    "       temp=le.fit_transform(test_l)\n",
    "       hot_labels_test=hot_labels_test.append(np_utils.to_categorical(temp,8))\n",
    "       (loss, accuracy)=model.evaluate(test_re, hot_labels_test,\n",
    "    \tbatch_size=1, verbose=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kerasRNN_batch():  \n",
    "   \n",
    "    featuers_train,labels_train,feauters_test,labels_test=w.prepare_data()\n",
    "    featuers_train=np.array(featuers_train)\n",
    "    feauters_test=np.array(feauters_test)\n",
    "   \n",
    "    \"\"\"\n",
    "    featuers_train,feauters_test,labels_train,labels_test =f.split_data(False)\n",
    "    featuers_train,feauters_test,feature_names=f.victorize( featuers_train,feauters_test)\n",
    "    \"\"\"\n",
    "    #dims=300\n",
    "    dims=300\n",
    "    #time_steps=265\n",
    "    #batch=1\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True,stateful=True,\n",
    "               batch_input_shape=(batch,None,dims)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(32, stateful=True,return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(dims))  # return a single vector of dimension 32\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(dims, return_sequences=True,\n",
    "               input_shape=(None,dims))) # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(64,return_sequences=True)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(32,return_sequences=True))) # return a single vector of dimension 32\n",
    "    model.add(TimeDistributed(Dense(8,activation='softmax')))\n",
    "\n",
    "             #model.add(Dense(8, activation='softmax'))\n",
    "   \n",
    "    #fit the model\n",
    "    print(\"fiting\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    count=0\n",
    "    train_data=[]\n",
    "    train_labels=[]\n",
    "    for sent_tr,label_tr in zip(featuers_train,labels_train):\n",
    "        hot_labels_train=[]\n",
    "        if(len(sent_tr)!=0):\n",
    "            train_re= np.reshape( sent_tr,(1,len(sent_tr),dims))\n",
    "            temp=le.fit_transform(label_tr)\n",
    "            hot_labels_train=np_utils.to_categorical(temp,8)\n",
    "            hot_labels_train=np.reshape(hot_labels_train,(1,len(hot_labels_train),8))\n",
    "            #model.train_on_batch(train_re,hot_labels_train)\n",
    "            train_data.append(train_re)\n",
    "            train_labels.append(hot_labels_train)\n",
    "        else:\n",
    "            count+=1\n",
    "    \n",
    "    \n",
    "    model.fit_generator(one_batch_generator(train_data,train_labels),steps_per_epoch=len(train_data), epochs=30, verbose=1)\n",
    "    \n",
    "    #test the model\n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "    \n",
    "    test_data=[]\n",
    "    test_labels=[]\n",
    "    for sent_te,label_te in zip(feauters_test,labels_test):\n",
    "        hot_labels_test=[]\n",
    "        if(len(sent_te)!=0):\n",
    "            test_re= np.reshape( sent_te,(1,len(sent_te),dims))\n",
    "            temp=le.fit_transform(label_te)\n",
    "            hot_labels_test=np_utils.to_categorical(temp,8)\n",
    "            hot_labels_test=np.reshape(hot_labels_test,(1,len(hot_labels_test),8))\n",
    "            test_data.append(test_re)\n",
    "            test_labels.append(hot_labels_test)\n",
    "          \n",
    "          #(loss, accuracy) = model.test_on_batch(test_re, hot_labels_test)\n",
    "\n",
    "    (loss, accuracy) = model.evaluate_generator(one_batch_generator(test_data,test_labels), steps=len(test_data), max_q_size=10, workers=1)\n",
    "\n",
    "    print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    \t\n",
    "    accuracy * 100))\n",
    "    \n",
    "    \n",
    "def one_batch_generator(X, y):\n",
    "    \n",
    "    while True:\n",
    "        for counter in range(len(X)):\n",
    "            X_batch = X[counter]\n",
    "            y_batch = y[counter]\n",
    "          \n",
    "            yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kerasRNNV():\n",
    "    dims=300\n",
    "    #dims=500\n",
    "    \"\"\"\n",
    "    featuers_train,feauters_test,labels_train,labels_test =f.split_data(False)\n",
    "    featuers_train,feauters_test,feature_names=f.victorize( featuers_train,feauters_test)\n",
    "   \"\"\"\n",
    "    featuers_train,labels_train,feauters_test,labels_test=w.prepare_data()\n",
    "    \n",
    "    \"\"\"\n",
    "    transformed_featuers_train=[item for sublist in transformed_featuers_train for item in sublist]    \n",
    "    labels_trainn=[item for sublist in labels_trainn for item in sublist]    \n",
    "    transformed_featuers_test=[item for sublist in transformed_featuers_test for item in sublist]    \n",
    "    labels_testn=[item for sublist in labels_testn for item in sublist]    \n",
    "    \"\"\"\n",
    "    featuers_train=np.reshape( featuers_train,(len(featuers_train),1,dims))\n",
    "    feauters_test=np.reshape( feauters_test,(len(feauters_test),1,dims))\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    transfoemed_labels_train = le.fit_transform(labels_train)\n",
    "    hot_labels_train = np_utils.to_categorical(transfoemed_labels_train,8)\n",
    "    transformed_labels_test=le.fit_transform(labels_test)\n",
    "    hot_labels_test = np_utils.to_categorical(transformed_labels_test, 8)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(dims, return_sequences=True,\n",
    "               input_shape=(None,dims)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(64,return_sequences=True)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(Bidirectional(LSTM(32))) # return a single vector of dimension 32\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    \n",
    "    #ftting model\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    #sgd = SGD(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam',\n",
    "    \tmetrics=[\"accuracy\"])\n",
    "    model.fit(featuers_train, hot_labels_train, nb_epoch=30, batch_size=1000,shuffle=False)\n",
    "    \n",
    "    #testing \n",
    "    print(\"[INFO] evaluating on testing set...\")\n",
    "    (loss, accuracy) =  model.evaluate(feauters_test, hot_labels_test,\n",
    "    \tbatch_size=300, verbose=1)\n",
    "    print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    \t\n",
    "    accuracy * 100))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kerasMLP()\n",
    "#kerasMLPV()\n",
    "#kerasRNN()\n",
    "#kerasRNN_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
