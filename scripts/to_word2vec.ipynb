{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from time import time\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import PADTparsing as p\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocacb 734610\n"
     ]
    }
   ],
   "source": [
    "unfound_words=[]\n",
    "found_words=[]\n",
    "sent_len=[]\n",
    "model=gensim.models.Word2Vec.load(os.path.realpath('..')+'/word2vecModels/word2vec_1000')\n",
    "print(\"vocacb\",len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    tagged_data=p.tagged_sents()\n",
    "    random.seed(5)\n",
    "    random.shuffle(tagged_data)\n",
    "    \n",
    "    sentences,labels=vec_data(tagged_data)  \n",
    "    \"\"\"if(flat):\n",
    "     sentences,labels=flat_sents,flat_labels\n",
    "    if(norm):\n",
    "     sentences=norm_vec_data(sentences)\"\"\"\n",
    "\n",
    "    #sentences,labels=pad(sentences,labels)\n",
    "    \n",
    "    #sentences,labels=clean(sentences,labels) \n",
    " \n",
    "    split_factor=int(.75*len(sentences))\n",
    "    sentences_train=sentences[:split_factor]\n",
    "    labels_train=labels[:split_factor]\n",
    "    sentences_test=sentences[split_factor:]\n",
    "    labels_test=labels[split_factor:]\n",
    "    return  sentences_train,labels_train,sentences_test,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_data(tagged_data):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    tagged_data=out_vocab(tagged_data)\n",
    "    #re_train(tagged_data)\n",
    "    model = gensim.models.Word2Vec.load(os.path.realpath('..')+'/word2vecModels/updated_model')\n",
    "    #model=gensim.models.Word2Vec.load('os.path.realpath('..')+'/word2vecModels/word2vec_1000')\n",
    "    print(\"new  vocacb\",len(model.wv.vocab))\n",
    "\n",
    "    for sent in tagged_data:\n",
    "        sentence=[]\n",
    "        label=[]\n",
    "        for idx,(tag,word) in enumerate(sent):\n",
    "            #if word  in model.wv.vocab:\n",
    "            word_vec=model[word]\n",
    "            sentences.append(word_vec)\n",
    "            labels.append(tag)\n",
    "           \n",
    "            \n",
    "        #if(len(sentence)!=0): \n",
    "        sent_len.append(len(sentence))   \n",
    "        #sentences.append(normalize(sentence, norm='max'))\n",
    "        #labels.append(label)\n",
    "    #sentences=standard_scaler.fit_transform(sentences)\n",
    "    sentences=normalize(sentences, norm='max')\n",
    "\n",
    "\n",
    "    return sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_vec_data(sentences):\n",
    "    n_sentences=normalize(sentences, norm='max')\n",
    "    return n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_train(tagged_data):\n",
    "    more_sent=[]\n",
    "    train_sent=[]\n",
    "    for sent in tagged_data:\n",
    "        for (tag,word) in sent:\n",
    "            if(word==\"OOV\"):\n",
    "                more_sent.append(sent)\n",
    "                break\n",
    "    for sent in more_sent:\n",
    "         train_sent.append([t[1] for t in sent])\n",
    "    print(len(train_sent))\n",
    "    print(train_sent[1])\n",
    "    \n",
    "    #model.build_vocab(train_sent, update=False)\n",
    "\n",
    "    model.train(train_sent)\n",
    "\n",
    "    model.build_vocab(train_sent, update=True)\n",
    "    model.train(train_sent)\n",
    "   \n",
    "    model.save(os.path.realpath('..')+'/word2vecModels/updated_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def out_vocab(tagged_data):\n",
    "    for sent in tagged_data:\n",
    "        for idx,(tag,word) in enumerate(sent):\n",
    "             if word not in model.wv.vocab:\n",
    "                    sent [idx]=(tag,\"OOV\")\n",
    "    return tagged_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_vec():\n",
    "    vec=[]\n",
    "    for i in range(300):\n",
    "        vec.append(2147483647)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word(w):\n",
    "    \n",
    "     return  normalize(model[w], norm='max').reshape(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(sentences,labels):\n",
    "    z_vec=zero_vec()\n",
    "    for idx,sent in enumerate(sentences):\n",
    "        for i in range(265-len(sent)):\n",
    "            sent.append(z_vec)\n",
    "            labels[idx].append(\"pad\")\n",
    "    return sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(sentences,labels):\n",
    "    for idx,sent in enumerate(sentences):\n",
    "         if(len(sent)==0):\n",
    "                count=count+1\n",
    "                del sentences[idx]\n",
    "                del labels[idx]\n",
    "    return sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt0 = time()\\nprint(\"start\")   \\nsentences_train,labels_train,sentences_test,labels_test=prepare_data()\\nprint (\"Training time:\", round(time()-t0, 3), \"s\")\\nprint(\"len train\",len(sentences_train),\"len test\",len(sentences_test))  \\nprint(\"not found\",len(unfound_words))\\nprint(\"found\",len(found_words))\\nprint(\"sent_len\",len(set(sent_len)))\\nprint(\"max of list\",max(sent_len))\\nprint(\"unfound words\",unfound_words) \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "t0 = time()\n",
    "print(\"start\")   \n",
    "sentences_train,labels_train,sentences_test,labels_test=prepare_data()\n",
    "print (\"Training time:\", round(time()-t0, 3), \"s\")\n",
    "print(\"len train\",len(sentences_train),\"len test\",len(sentences_test))  \n",
    "print(\"not found\",len(unfound_words))\n",
    "print(\"found\",len(found_words))\n",
    "print(\"sent_len\",len(set(sent_len)))\n",
    "print(\"max of list\",max(sent_len))\n",
    "print(\"unfound words\",unfound_words) \n",
    "\"\"\"\n",
    "#used to retainr the model with words that our present in PADT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
